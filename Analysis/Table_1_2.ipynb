{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3340d4a0",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c7df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re \n",
    "import progressbar as pb\n",
    "\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import string\n",
    "from jieba import posseg as pseg\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"Dataset for Table_1_2, Fig_4.xlsx\")\n",
    "\n",
    "all_visible = df.loc[df[\"fulltext_released_to_public\"] == True]\n",
    "all_invisible = df.loc[df[\"fulltext_released_to_public\"] == False]\n",
    "\n",
    "visible_list = all_visible[\"title\"].values.tolist()\n",
    "invisible_list = all_invisible[\"title\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9dddc",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4874577b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\vbrus\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.624 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def strip_text(text):\n",
    "    \"\"\"\n",
    "    strips a Chinese text to only the desired features\n",
    "    \n",
    "    args:\n",
    "        text: list of strings to clean\n",
    "    \n",
    "    returns:\n",
    "        text: list of strings that have been cleaned\n",
    "    \n",
    "    \"\"\"\n",
    "    sentence = [re.sub (\"\\/\", \"_\", str (item)) for item in pseg.cut(text)] \n",
    "    sentence = [word for word in sentence if not re.search (\"_ns|_x|_m\", word)] #exclude place names, non-morphemes, measures \n",
    "    sentence = [word for word in sentence if len (re.sub (\"_.*\", \"\", word))>1] #restrict to two-character words\n",
    "    sentence = [word for word in sentence if re.search (\"_n|_v|_j\", word)] #restrict to nouns, verbs, adjectives,  \n",
    "    return ' '.join(list(re.sub (\"_.*\", \"\", word) for word in sentence))\n",
    "    #return (list(re.sub (\"_.*\", \"\", word) for word in sentence))#delete POS tags\n",
    "\n",
    "v = [strip_text(x) for x in visible_list]\n",
    "inv = [strip_text(x) for x in invisible_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25686f5f",
   "metadata": {},
   "source": [
    "# Fightin' Words algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304e93c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 4799\n",
      "Comparing language...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "import string\n",
    "\n",
    "def bayes_compare_language(l1, l2, ngram = 1, prior=.01, cv = None):\n",
    "    '''\n",
    "    args:\n",
    "      l1, l2; a list of strings from each language sample\n",
    "      ngram; an int describing up to what n gram you want to consider (1 is unigrams,\n",
    "    2 is bigrams + unigrams, etc). Ignored if a custom CountVectorizer is passed.\n",
    "      prior; either a float describing a uniform prior, or a vector describing a prior\n",
    "    over vocabulary items. If you're using a predefined vocabulary, make sure to specify that\n",
    "    when you make your CountVectorizer object.\n",
    "      cv; a sklearn.feature_extraction.text.CountVectorizer object, if desired.\n",
    "\n",
    "    returns:\n",
    "      A list of length |Vocab| where each entry is a (n-gram, zscore) tuple.\n",
    "    '''\n",
    "    \n",
    "    if cv is None and type(prior) is not float:\n",
    "        print(\"If using a non-uniform prior:\")\n",
    "        print(\"Please also pass a count vectorizer with the vocabulary parameter set.\")\n",
    "        quit()\n",
    "    if cv is None:\n",
    "        cv = CV(decode_error = 'ignore', min_df = 10, max_df = .5, ngram_range=(1,ngram),\n",
    "                binary = False,\n",
    "                max_features = 15000)\n",
    "    counts_mat = cv.fit_transform(l1+l2).toarray()\n",
    "    # Now sum over languages...\n",
    "    vocab_size = len(cv.vocabulary_)\n",
    "    print(\"Vocab size is {}\".format(vocab_size))\n",
    "    if type(prior) is float:\n",
    "        priors = np.array([prior for i in range(vocab_size)])\n",
    "    else:\n",
    "        priors = prior\n",
    "    z_scores = np.empty(priors.shape[0])\n",
    "    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n",
    "    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n",
    "    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n",
    "    a0 = np.sum(priors)\n",
    "    n1 = 1.*np.sum(count_matrix[0,:])\n",
    "    n2 = 1.*np.sum(count_matrix[1,:])\n",
    "    print(\"Comparing language...\")\n",
    "    for i in range(vocab_size):\n",
    "        #compute delta\n",
    "        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n",
    "        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))        \n",
    "        delta = term1 - term2\n",
    "        #compute variance on delta\n",
    "        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n",
    "        #store final score\n",
    "        z_scores[i] = delta/np.sqrt(var)\n",
    "    index_to_term = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    sorted_indices = np.argsort(z_scores)\n",
    "    return_list = []\n",
    "    for i in sorted_indices:\n",
    "        return_list.append((index_to_term[i], z_scores[i]))\n",
    "    return return_list\n",
    "\n",
    "results = pd.DataFrame(bayes_compare_language(v, inv), columns=[\"Word\", \"Score\"])\n",
    "results.to_excel(\".//results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b45d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
